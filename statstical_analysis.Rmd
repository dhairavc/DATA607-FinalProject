---
title: "DATA607 Final Project"
author: "Salma Elshahawy, Mael Illien, Dhairav Chhatbar"
date: "11/18/2019"
output: 
  html_document:
    theme: united
    df_print: paged
    toc: true
    toc_float: 
      collapsed: true
      smooth_scroll: true
    number_sections: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# A Socio-Economic Investigation into Crime

Presented by:

+ Salma Elshahawy [salma71](https://github.com/salma71)
+ Dhairav Chhatbar [dhairavc](https://github.com/dhairavc)
+ Mael Illien [maelillien](https://github.com/maelillien) 

## Introduction

This project provided us with the opportunity of showcasing many of the skills we have learned throughout this course and of applying them to an investigation into datasets of our choosing. We narrowed our scope to a few datasets containing information on socio-economic information, namely unemployment and crime data in NYC. Specifically we wanted to explore the impact of unemployment on crime within New York City. The guiding question for our statistical analysis if the following hypothesis:

\[H0: Unemployment \ does \ not \ impact \ crime\ (R^2 = 0) \]
\[HA: Unemployment \ does \ impact \ crime\ (R^2 > 0) \]


We hoped that this investigation alongside exploratory data analysis would provide us with useful information that could be used to formulate policy proposals. We used the following process for each dataset:	

1. Data Import (API and .csv)	
2. Data Transformation (handling missing data, tidying)	
3. Data Exploration & Analysis (commentary, visualizations)	

We then merged the datasets to explore further and try to draw some final conslusions based on the resulting model.

## Work flow-chart

![workflowchart](./workflowchart.png)

## Sources of Datasets

- [NYPD Arrests Data Historic](https://data.cityofnewyork.us/Public-Safety/NYPD-Arrests-Data-Historic-/8h9b-rp9u)
- [Unemployment data from the Department of Labor](https://www.labor.ny.gov/stats/nyc/)


## Environment Setup

The packages are loaded from a separate R file.

```{r environment setup, message=FALSE, warning=FALSE}
source("environment_setup.R", echo = T, prompt.echo = "", spaced = F)
```

# NYPD Arrests

We will start with the NYPD Arrests Data (Historic) data from NYC Open Data found below and conduct some exploratory data analysis to find out how arrests are distributed in general. We will explore demographic trends as well as trends in particular kinds of arrest or by boroughs.

This dataset provides observations of confirmed (individuals NOT acquitted of all charges) crimes as recorded New York City Police Department (NYPD).  

## NYPD Arrests - Data Import via API

There are 4.8M rows, there are 18 columns and each row is an arrest.

variable         | description
---------------- | -----------
`arrest_date`    | Exact date of arrest for the reported event.
`ofns_desc`      | Description of internal classification corresponding with KY code (more general category than PD description).
`arrest_boro`    | Borough of arrest. B(Bronx), S(Staten Island), K(Brooklyn), M(Manhattan), Q(Queens)
`age_group`      | Perpetrator’s age within a category.
`perp_sex`       | Perpetrator’s sex description.
`perp_race`      | Perpetrator’s race description.
`x_coord_cd`     | Midblock X-coordinate for New York State Plane Coordinate System, Long Island Zone, NAD 83, units feet (FIPS 3104).
`y_coord_cd`     | Midblock Y-coordinate for New York State Plane Coordinate System, Long Island Zone, NAD 83, units feet (FIPS 3104)
`latitude`       | Latitude coordinate for Global Coordinate System, WGS 1984, decimal degrees (EPSG 4326)
`longitude`      | Longitude coordinate for Global Coordinate System, WGS 1984, decimal degrees (EPSG 4326)

The loaded R script will read the data into R using the RSocrata API.

```{r arrests, warning=FALSE, message=FALSE}
source("arrests_dataset.R", echo = F, prompt.echo = "", spaced = F)
head(arrests_df, 10)
```

## NYPD Arrests - Data Transformation

Rename the borough letters to proper names.

```{r change boro names, message=FALSE, warning=FALSE}
arrests_df$arrest_boro <- revalue(arrests_df$arrest_boro, c("Q"="Queens", "K"="Brooklyn", "M"="Manhattan", "S"="Staten Island", "B" = "Bronx")) 
```

Remove missing values where no offense description is recorded.

```{r remove empty ofns, warning=FALSE, message=FALSE}
arrests_df <- arrests_df %>% filter(ofns_desc != "")
```

We generate a series of data frames aggregating the data in different manners for analysis and plotting. For example, we look at arrests by race, borough, offense.

```{r aggregation, message=FALSE, warning=FALSE}
murder_counts <- arrests_df %>%
  group_by(arrest_boro, year, perp_race) %>%
  dplyr::summarise(murder_counts = n()) %>%
  arrange(desc(year))
murder_counts
```

```{r arrests counts, message=FALSE, warning=FALSE}
# get the count of arrests per year, by borough
grouped_boro <- arrests_df %>% 
  group_by(year, arrest_boro) %>% 
  dplyr::summarize(count = n()) %>% 
  arrange(desc(count))
```

```{r ofnse counts, message=FALSE, warning=FALSE}
# get the count of offenses per year, by borough
grouped_offenses <- arrests_df %>% 
  group_by(year, arrest_boro, ofns_desc) %>% 
  dplyr::summarize(count = n()) %>%
  arrange(desc(count))
```

```{r top five ofnse, message=FALSE, warning=FALSE}
# get the top five offense per borough
t5 <- grouped_offenses %>% top_n(5)
```

```{r counts ofns overall, message=FALSE, warning=FALSE}
# get the counts of offenses overall
crime_counts <- arrests_df %>% 
  group_by(ofns_desc) %>% 
  dplyr::summarize(count = n()) %>% 
  arrange(desc(count))
```

```{r arrests count relative to dangerous drugs, message=FALSE, warning=FALSE}
# get the count of arrests related to dangerous drugs by year, by borough
drugs <- arrests_df %>% 
  filter(ofns_desc == 'DANGEROUS DRUGS') %>% 
  group_by(year, arrest_boro) %>% 
  dplyr::summarize(count = n())
```

## NYPD Arrest - Data Exploration & Analysis

Let's study the evolution of crime over the period of interest (2014-2018).

What the plot below reveals is that overall crime is decreasing for all boroughs of NYC. The data year over year is very similar, appearing to simply scale down over time. 

```{r crimes by boro time series, echo=FALSE, message=FALSE, warning=FALSE}
ggplot(grouped_boro, aes(x = reorder(year, -count), y = count, fill = arrest_boro)) + 
  geom_bar(stat = 'identity', position = position_dodge()) +
  scale_y_continuous(labels=function(x) format(x, big.mark = ",", scientific = FALSE), breaks = seq(0,120000,10000)) +
  xlab("year") + ylab("total crime") + ggtitle("Crime by Borough Time Series") +
  scale_fill_brewer(palette="Blues") + theme_minimal()
```

What we can note as suprising is the fact that total crime between Manhattan and Brooklyn is at fairly similar levels. Total crime is aggregated without accounting for different types of crime so we will further our investigation by looking at top crimes overall, and then dissecting crime per borough.

Here is a plot of the top 10 most common crimes for the 2014-2018 period across all boroughs. We learn that dangerous drugs related offenses are the most prevailent followed by 3rd degree assaults.

```{r most common crimes, echo=FALSE, message=FALSE, warning=FALSE}
ggplot(crime_counts %>% top_n(10), aes(x = reorder(ofns_desc, -count), y = count)) + 
  geom_bar(stat = 'identity', fill= 'lightblue') + 
  scale_y_continuous(labels=function(x) format(x, big.mark = ",", scientific = FALSE), breaks = seq(0,300000,50000)) +
  geom_text(aes(label = count), hjust = 1.25, color='white') +
  coord_flip() + 
  xlab("offense") + ylab("count")  + ggtitle("Most Common Crimes 2014-2018") +
  theme(axis.text.x = element_text(size=10), axis.text.y = element_text(size=8))
```

A peek at the bottom 10 crimes for the same period reveals somewhat unexpected crimes like disruption of a religious service. It is interesting to note that while dangerous drugs offenses are the most common crime, only 1 person was arrested for being under the influence of drugs.

```{r least commo crimes, message=FALSE, warning=FALSE, echo=FALSE}
ggplot(crime_counts %>% top_n(-10), aes(x = reorder(ofns_desc, -count), y = count)) + 
  geom_bar(stat = 'identity', fill= 'lightblue') + 
  scale_y_continuous(labels=function(x) format(x, big.mark = ",", scientific = FALSE), breaks = seq(0,75,5)) +
  geom_text(aes(label = count), hjust = 1.25, color='white') +
  coord_flip() + 
  xlab("offense") + ylab("count")  + ggtitle("Least Common Crimes 2014-2018") +
  theme(axis.text.x = element_text(size=10), axis.text.y = element_text(size=8))
```

Following from  the exploration above, we take a deeper look at the most common crimes by borough. On the plot below, we once again see that how drug related offenses are the most common crimes and that this is consistent across boroughs. We notice that while Brooklyn and Manhattan had the most crimes, the Bronx captures the most drug arrests.

```{r top 5 crimes by boro, message=FALSE, warning=FALSE, echo=FALSE}
ggplot(t5, aes(x = reorder(arrest_boro, -count), y = count, fill=ofns_desc)) + 
  geom_bar(stat = 'identity', position = position_dodge()) +
  scale_y_continuous(labels=function(x) format(x, big.mark = ",", scientific = FALSE), breaks = seq(0,80000,5000)) +
  xlab("borough") + ylab("crime rate") + ggtitle("Top 5 Crimes by Borough") +
  scale_fill_brewer(palette="Blues") + theme_minimal()
```

The plot below explores that relationship over time for each borough. We observe that similarly to crime in general, drug related arrests are going down.

```{r dangerous drug crime, echo=FALSE, message=FALSE, warning=FALSE}
ggplot(drugs, aes(x = year, y = count, color = arrest_boro)) +
  geom_line() + 
  scale_y_continuous(labels=function(x) format(x, big.mark = ",", scientific = FALSE), breaks = seq(0,30000,2500)) +
  xlab("year") + ylab("count")  + ggtitle("Dangerous Drugs Crime by Bourough") +
  theme(axis.text.x = element_text(size=10), axis.text.y = element_text(size=8))
  
```

We continue investigating the demographics and take a look at the distribution of crime by gender. Male adults between the ages of 25-44 remain the most common perpetrators. This age group is also the common for female perpetrators.

```{r age group gender distribution, message=FALSE, warning=FALSE, echo=FALSE}
ggplot(arrests_df, aes(x = age_group, fill = perp_sex)) + 
  geom_histogram(stat = "count", position=position_dodge()) +
  scale_fill_brewer(palette="Blues") + 
  xlab("age group") + ylab("count")  + ggtitle("Perpetrator Age Group and Gender Distribution") +
  scale_y_continuous(labels=function(x) format(x, big.mark = ",", scientific = FALSE), breaks = seq(0,7000000,100000))
```

By exploring age group futher by borough, we can see that the 25-44 age category is also the largest age category density for committing crimes across all boroughs. As also shown below, the Bronx seems to have the highest density of crimes for all age categories across boroughs. 

```{r density plot for age, message=FALSE, warning=FALSE, echo=FALSE, fig.width=10}
theme_set(theme_classic())
# Plot
g <- ggplot(arrests_df, aes(arrest_boro))
g + geom_density(aes(fill=factor(age_group)), alpha=0.8) + 
    labs(title="Density plot", 
         subtitle="Number of crimes per boro per age group distribution",
         caption="Source: arrests_df",
         x="Borough",
         fill="Age group") 
```

The treemap below was used to study which perpetrator race is the most common by borough. As illustrated, BLACK is the greatest proportion. However, Brooklyn appears to have more crimes committed by BLACK than the Bronx. 

```{r treemap for arrests, message=FALSE, warning=FALSE, echo=FALSE}
map <- murder_counts %>% filter(year == 2018)
treemap(map, #Your data frame object
        index=c("perp_race","arrest_boro"),  #A list of your categorical variables
        vSize = "murder_counts",  #This is your quantitative variable
        type="categorical", #Type sets the organization and color scheme of your treemap
        vColor = "arrest_boro", #Type sets the organization and color scheme of your treemap
        palette = "Set1",  #Select your color palette from the RColorBrewer presets or make your own.
        title="Crime distribution committed by different races - year 2018", #Customize your title
        fontsize.title = 14 #Change the font size of the title
        ) 
```

To take the investigation even further, this interactive map will let you explore the distribution of crime geographically.  

```{r map for arrests,echo=FALSE, fig.align='center', message=FALSE, warning=FALSE}
arrests_map <- arrests_df %>% filter(year == 2018)
singleicon <- makeIcon(iconUrl = "https://image.flaticon.com/icons/svg/1331/1331396.svg",
                       iconWidth = 45,
                       iconHeight = 45,
                       iconAnchorX = 0,
                       iconAnchorY = 0)
leaflet(arrests_map, width = '100%') %>% addTiles() %>% 
                                  addMarkers(lng = ~longitude, lat = ~latitude, 
                                             clusterOptions = markerClusterOptions(), 
                                             popup = paste("<b>Offense: </b>", arrests_map$ofns_desc, "<br/>",
                                                           "<b>Age Group: </b>", arrests_map$age_group, "<br/>",
                                                           "<b>Sex: </b>", arrests_map$perp_sex, "<br/>",
                                                           "<b>Race: </b>", arrests_map$perp_race),
                                             icon = singleicon)
```

# Labor Bureau

We wanted to investigate if there is a correlation between crimes and unemployment rate. So we extended our analysis with another dataset from the labor bureau. The dataset has five tables for the five boroghs for a  period of four years from 2014 - 2018. 

This dataset provides socio-economic observations like unemployed population and the unemployment rate of the active labor force.

## Labor Bureau - Data Import via .csv

We load the script below to import the data into R. Here is a snapshot of the data for Manhattan.

```{r dataset per boro, message=FALSE, warning=FALSE, paged.print=TRUE}
source("unemployed_dataset.R", echo = F, prompt.echo = "", spaced = F)
head(manhattan)
```

## Labor Bureau - Data Transformation

We perfomed typical data transformation operations to clean the table. This involves removing missing values and renaming columns. 

```{r clean, message=FALSE, warning=FALSE}
clean_table <- function (table) {
  table_content <- table %>%
    na.omit()
  colnames(table_content) = c("arrest_boro","year","month","labor_force","employed","unemployed","unemployment_rate")
  final_table <- table_content %>%
    select(arrest_boro, year, labor_force, employed, unemployed, unemployment_rate) 
  return(final_table)
}
```

```{r clean unemployed, message=FALSE, warning=FALSE}
bronx_income <- clean_table(bronx)
queens_income <- clean_table(queens)
brooklyn_income <- clean_table(brooklyn)
manhattan_income <- clean_table(manhattan)
staten_income <- clean_table(staten)
income_table <- Reduce(function(...) merge(..., all=T), list(bronx_income, queens_income, brooklyn_income, manhattan_income, staten_income))
income_table 
```

```{r summarize avg unemployed, message=FALSE, warning=FALSE}
by_income <- income_table %>%
  group_by(arrest_boro, year) %>%
  dplyr::summarise(avg_unemployment_rate = max(unemployment_rate)) %>%
  arrange(desc(year))
by_income
```

## Labor Bureau - Data Exploration & Analysis

The following boxplot shows the unemployment rate distribution per borough per year. We can see that there is a decreasing trend in the unemployment rate over the years. However, the Bronx county seems to have the highest unemployment rate over the years and it remains the highest in 2018 with a range of 5 - 6%.

```{r explore income by boro, message=FALSE, warning=FALSE, echo=FALSE}
theme_set(theme_bw())
ggplot(income_table, aes(x=arrest_boro, y=unemployment_rate)) + 
    geom_boxplot(main = "Different boxplot for uneployment rate in the 5 counties over years",
        ylab = "Unemployment rate %",	
        xlab = "",
        col = "blue",
        border = "blue") +
facet_wrap(~year, scale="free") + 
  theme(axis.text.x = element_text(angle=60, vjust = 0.6)) +
  ggtitle("Unemployment rate dsitributed by year and by borough")
```

# Combined Datasets

After understanding our datasets individually, we merged them into a single data frame based on the variable that were shared between the two sources, namely `year` and `arrest_boro`. This allowed us to extend our analysis to the variables in the combined dataset and to study how they behave in relation to each other.

```{r merge crime and by income, message=FALSE, warning=FALSE}
merged <- Reduce(function(...) merge(..., all=T), list(murder_counts, by_income)) %>%
  na.omit() %>%
  arrange(desc(year))
merged
```

## Combined - Data Transformation

```{r fetch complaindata 14-18, message=FALSE, warning=FALSE}
source("complain_dataset.R", echo = F, prompt.echo = "", spaced = F)
dat
```
```{r aggregate crime number, message=FALSE, warning=FALSE, paged.print=TRUE}
res <- dat %>%
  group_by(year, boro_nm) %>%
  filter(boro_nm != "") %>%
  dplyr::summarise(crimes = n()) 
res
```

```{r change to lowercase, message=FALSE, warning=FALSE, paged.print=TRUE}
#by_crime <- res %>% mutate_if(is.character, str_to_lower) 
by_crime <- arrests_df %>% select(year, arrest_boro) %>% group_by(year, arrest_boro) %>% dplyr::summarise(crimes = n())
colnames(by_crime) <- c("year", "boro_nm", "crimes" )
by_crime
```
```{r filter by unemployed, message=FALSE, warning=FALSE, paged.print=TRUE}
dat2 <- income_table
dat2$unemployed <- as.numeric(gsub(",", "", dat2$unemployed))
names(dat2)[1] <- "boro_nm"
subet <- dat2[c(1:2,5)]
by_unemployment <- subet %>%
  group_by(year, boro_nm) %>%
  dplyr::summarise(unemployed = round(mean(unemployed), 2)) #%>%
  #mutate_if(is.character, str_to_lower)
by_unemployment$boro_nm <- revalue(by_unemployment$boro_nm, c("Queens "="Queens"))
by_unemployment
```

```{r merge crime and unemployed, message=FALSE, warning=FALSE, paged.print=TRUE}
#joined <- Reduce(function(...) merge(..., all=T), list(by_crime, by_unemployment)) 
joined <- left_join(by_crime, by_unemployment, by = c("year"="year", "boro_nm"="boro_nm"))
joined
```

```{r aggregate by category, message=FALSE, warning=FALSE}
by_cat <- arrests_df %>% select(year, arrest_boro, age_group, perp_sex, perp_race) %>% group_by(year, arrest_boro, age_group, perp_sex, perp_race) %>% dplyr::summarise(crimes=n())
colnames(by_cat) <- c("year", "boro_nm", "susp_age_group", "susp_sex", "susp_race", "crimes")
by_cat
```

Categorize age-group preparing for dummy variables

```{r construct dummy variables, message=FALSE, warning=FALSE}
dummy_df <- by_cat
dummy_df$susp_age_group <- revalue(by_cat$susp_age_group, c("<18"="child", "18-24"="youth", "25-44"="adult", "45-64"="senior", "65+" = "senior")) 
dummy_df
```

Converting the independent variables into factors

```{r convert variables into factors, message=FALSE, warning=FALSE}
#function to categorize and indicate the cofounder variable
#
# function to categorise - dummy variables
filtered <- dummy_df %>%
  filter(susp_age_group %in% c("child", "youth", "adult", "senior")) 
## change to factor level
filtered$boro_nm <- as.factor(filtered$boro_nm)
filtered$susp_age_group <- as.factor(filtered$susp_age_group)
filtered$susp_sex <- as.factor(filtered$susp_sex)
filtered$susp_race <- as.factor(filtered$susp_race)
# contrasts(filtered$susp_race)
# reference variable is AMERICAN INDIAN/ALASKAN NATIVE, we can rereference by using relevel(var, ref = "new_ref")
```

## Combined - Data Exploration & Analysis

We tried to plot a diverging plot to investigate which borough is above or below average. So we normalized the average unemployment rate and the number of crimes. Both Brooklyn and Bronx seem to have the most significant above average for crimes committed and the unemployment rate. 

```{r normalize unemployed, message=FALSE, warning=FALSE, echo=FALSE}
# Data Prep
merged$boro <- rownames(merged)  # create new column for boro names
merged$unemployment_z <- round((merged$avg_unemployment_rate - mean(merged$avg_unemployment_rate))/sd(merged$avg_unemployment_rate), 2)  # compute normalized 
merged$unemployyment_type <- ifelse(merged$unemployment_z < 0, "below", "above")  # above / below avg flag
merged <- merged[order(merged$unemployment_z), ]  # sort
# merged$arrest_boro <- factor(merged$arrest_boro, levels = merged$arrest_boro)  # convert to factor to retain sorted order in plot.
merged$arrest_boro <- factor(merged$arrest_boro, levels = rev(unique(merged$arrest_boro)), ordered=TRUE)
# Diverging Barcharts
ggplot(merged, aes(x=`arrest_boro`, y=unemployment_z, label=unemployment_z)) + 
  geom_bar(stat='identity', aes(fill=unemployyment_type), width=.5)  +
  scale_fill_manual(name="Unemployment", 
                    labels = c("Above Average", "Below Average"), 
                    values = c("above"="#f8766d", "below"="#00ba38")) + 
  labs(subtitle="Normalised unemployment rate from 'merged'", 
       title= "Diverging Bars of unemployment rates in boro") + 
  coord_flip()
```

```{r normalize crime, message=FALSE, warning=FALSE, echo=FALSE}
# Data Prep
merged$boro <- rownames(merged)  # create new column for boro names
merged$crime_z <- round((merged$murder_counts - mean(merged$murder_counts))/sd(merged$murder_counts), 2)  # compute normalized 
merged$murder_type <- ifelse(merged$crime_z < 0, "below", "above")  # above / below avg flag
merged <- merged[order(merged$crime_z), ]  # sort
# merged$arrest_boro <- factor(merged$arrest_boro, levels = merged$arrest_boro)  # convert to factor to retain sorted order in plot.
merged$arrest_boro <- factor(merged$arrest_boro, levels = rev(unique(merged$arrest_boro)), ordered=TRUE)
# Diverging Barcharts
ggplot(merged, aes(x=`arrest_boro`, y=crime_z, label=crime_z)) + 
  geom_bar(stat='identity', aes(fill=murder_type), width=.5)  +
  scale_fill_manual(name="Crimes", 
                    labels = c("Above Average", "Below Average"), 
                    values = c("above"="#f8766d", "below"="#00ba38")) + 
  labs(subtitle="Normalised crimes'", 
       title= "Diverging Bars of number of crimes per county") + 
  coord_flip()
```

# Statistical Analysis & Modeling

To answer the initial hypothesized question, the combined dataset is utilized in a linear regression modeling exercise to understand the relationship between the variables. In this process we were able to identify which variables are the greatest predictors of the number of crimes commited.

We will also verify the conditions for inference for linear modeling.

## Linear Regression

To start building the predictive model, we need to see if there is a correlation between our predictor and response. In this case, crimes and unemployed. We will begin by doing some exploratory data visualization to verify the conditions for inference. The function `ggpairs()`  from the `GGally package` was used to create a plot matrix demonstrating how the variables relate to one another.  

```{r correlation between crime and unemployed, message=FALSE, warning=FALSE, echo=FALSE}
ggpairs(data=joined, columns = 3:4, title="Crimes employed data")
```

There is essential regard that correlation doesn't imply causation, so constructing a regression model is imperative to comprehend whether we can use this variable as a predictor. 

We started by scrutinizing the relationship between the outcome and covariant. In our case, it is the number of crimes committed and the unemployment rate. It seems that their relationship is linear.

```{r explore-unemployed as predictor, message=FALSE, warning=FALSE, echo=FALSE}
theme_set(theme_bw())  # pre-set the bw theme.
g <- ggplot(joined, aes(unemployed, crimes)) + 
  geom_jitter() +
  geom_smooth(method="lm") +
  scale_y_continuous(labels=function(x) format(x, big.mark = ",", scientific = FALSE), breaks = seq(0,150000,25000)) +
  labs(y="Crimes", 
       x="Unemployed Population", 
       title="Crimes vs Unemployed Population", 
       caption="Source: joined")
ggMarginal(g, type = "boxplot", fill="transparent")
```
```{r unemployed regression, message=FALSE, warning=FALSE}
m_unemployed <- lm(crimes ~ unemployed, data = joined)
summary(m_unemployed)
```

As pointed by the simple linear regression, unemployment has an **R-squared  ~0.75**. This makes it a better predictor of the number of crimes. 
However, to make sure that our predictor model is reliable, we need to see if the residuals are so close to the actual value. This would give us a quality indicator of how is our prediction is fitting. 

```{r message=FALSE, warning=FALSE, echo=FALSE}
plot(m_unemployed, which=2, col=c("red"))  # Q-Q Plot
```

For our model, **the Q-Q plot** testing the normality of the residuals shows pretty good alignment to the the line with a few points at the top slightly offset indicating some skew. 

```{r message=FALSE, warning=FALSE, echo=FALSE}
plot(m_unemployed, which=3, col=c("blue")) # Scale-Location Plot
```

The residuals are not reasonably well spread above and below a pretty non horizontal line. This may raise a concern for us, this means that the relationship between the two variables are not linear. 

The model below based on race only explains ~20% of the variance in the data but the predictors of black, white and white hispanic are significant.

```{r change reference to M, message=FALSE, warning=FALSE}
m_race <- lm(crimes ~ susp_race, data = filtered)
summary(m_race) # no significance
```

To improve our model we investigated further by building a mutlivariable regression model taking other variables into account.

## Multiple Regression

We will start with some exploratory analysis on the filtered dataset.

Having some information shown on plots would give a good idea as to which categorical variables are good predictive features and can be used to build a machine learning model.
Best plots for factor to factor variable comparison would be any of a jitter plot or heat map. I would use a jitter plot in this case for all of our factor-factor plots.

```{r exploratory analysis, warning=FALSE, message=FALSE, echo=FALSE, fig.height=6}
ggplot(data = filtered, mapping = aes(x = susp_race, y = crimes)) + 
  geom_jitter(aes(colour = susp_race)) +
  theme(axis.text.x = element_text(angle=90, vjust=0.6))
ggplot(data = filtered, mapping = aes(x = susp_sex, y = crimes)) + 
  geom_jitter(aes(colour = susp_sex)) 
ggplot(data = filtered, mapping = aes(x = susp_age_group, y = crimes)) + 
  geom_jitter(aes(colour = susp_age_group)) 
ggplot(data = filtered, mapping = aes(x = boro_nm, y = crimes)) + 
  geom_jitter(aes(colour = boro_nm)) + 
  theme(axis.text.x = element_text(angle=65, vjust=0.6))
```

We then took other variables (susp_age_group, susp_sex, and boro_nm) into account.

```{r regression summary, warning=FALSE, message=FALSE}
m_all <- lm(crimes ~ susp_age_group + susp_sex + susp_race + boro_nm, data = filtered)
summary(m_all)
```

It can be seen that other factors remaining the same, **Black** is associated with an average increase of 3219 in crime rates compared to other races. 

We study the residuals to verify the validity of our model and similarly to when we used the unemployed variable, we have concerns about the residuals distribution revealed by the **Q-Q plot**. 

```{r qqplot, message=FALSE, warning=FALSE, echo=FALSE}
plot(m_all, which=2, col=c("red"))  # Q-Q Plot
```

## ANOVA

Given that that for our model we had an Adjusted R-squared: 0.4175, this would indicate that the model does explain 41.75% of the variability.  
To see if this amount is significant, we analyze the variances. Since the p-values of the variances are near zero,  
**we have sufficient evidence to reject the null hypothesis, in that we can say that unemployment does impact crime**

```{r}
anova(m_all)
```

# Conclusion

## Summary of findings

Our investigation allows us to make a number of findings:  

1. While crime overall is decreasing in NYC, black individuals are still disportionaly inclined to commit more crimes. The largest coefficent in the regression model is where the suspect race is black, 3219.82. 

2. The Bronx remains the borough with the highest unemployment rate, but Brooklyn and Manhattan have the highest crime rates of the 5 boroughs. 

3. The 25-44 age groups is the most likely to commit crimes for both genders.  

3. At a macro level, each increase in unemployment by one, results in a crime increase of 1.2209. At a more micro level, being a black or white/hispanic male is a good predictor of a crime.  
    
5. We built a geographic map of tghe distribution of commited crimes per borough.  

6. With an R-squared of ~75% we found that unemployment is indeed a good predictor of crime.  

7. With an R-squared of ~42% we found that our multivariable model explains some of the variability in the data but the significant skew should make us cautious.  

## Why is this important?

1. This kind of study is important in evaluating the success of policies over time by seeing the impact and relationships between crime and particular demographics. 

2. It can also be used to shape the future of police enforcement policy by allowin the department to better use their resources to tackle most common crimes where they are more likely to be concentrated.  

3. By merging crimes obersvations with socio-economic factor, we can identify connections to other variables like unemployment and shape policy by arguing that an investment in labor resources could have a significant beneficial impact on crime.  

## Future Work

Future work would involve collecting more variables for demographics and socio economic conditions like health and eduction and building a more encompassing model to predict crime rates. 

## Challenges

Throughout this assignment, we encountered a series of challenges like the following:

1. Working with very large data sets and we filtering them down
2. Sourcing external scripts into Rmarkdown to speed up knitting
3. Merging datasets with no distinct identifiers across each obsrvations  
4. Merging dataset with different inconsistent observation values
5. Determining which set of variables are a better fit for regression modeling.

## References 

+ [GitHub Repo](https://github.com/dhairavc/DATA607-FinalProject)